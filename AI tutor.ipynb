{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e98affd-eda0-4361-8db6-1320d9aef86b",
   "metadata": {},
   "source": [
    "I decided to make an AI tutor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6f23486-ae52-4d48-a1a7-9f4780f4105e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fccb242-3cad-4aba-b548-f686c9b40cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key looks good so far\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if api_key and api_key.startswith('sk-proj-') and len(api_key) > 10:\n",
    "    print(\"API key looks good so far\")\n",
    "else:\n",
    "    print(\"There might be a problem with your API key? Please visit the troubleshooting notebook!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd1e2f76-ccf4-479e-a6f6-0c699c3ed192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8ecf762-68c0-4b97-8f0d-c159bb8126dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are the software engineer, phd in mathematics, machine learning engineer, and other topics\"\"\"\n",
    "system_prompt += \"\"\"\n",
    "When responding, always use Markdown for formatting. For any code, use well-structured code blocks with syntax highlighting,\n",
    "For instance:\n",
    "```python\n",
    "\n",
    "sample_list = [for i in range(10)]\n",
    "```\n",
    "Another example\n",
    "```javascript\n",
    "        function displayMessage() {\n",
    "            alert(\"Hello, welcome to JavaScript!\");\n",
    "        }\n",
    "\n",
    "```\n",
    "\n",
    "Break down explanations into clear, numbered steps for better understanding. \n",
    "Highlight important terms using inline code formatting (e.g., `function_name`, `variable`).\n",
    "Provide examples for any concepts and ensure all examples are concise, clear, and relevant.\n",
    "Your goal is to create visually appealing, easy-to-read, and informative responses.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a40f3db8-6b81-4a8f-86f0-5df1d0ba7b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tutor_user_prompt(question):\n",
    "    # Ensure the question is properly appended to the user prompt.\n",
    "    user_prompt = (\n",
    "        \"Please carefully explain the following question in a step-by-step manner for clarity:\\n\\n\"\n",
    "    )\n",
    "    user_prompt += question\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac4e050e-60d6-46ea-8d71-4c11a08986c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def askTutor(question, MODEL):\n",
    "    # Generate the user prompt dynamically.\n",
    "    user_prompt = tutor_user_prompt(question)\n",
    "    \n",
    "    # OpenAI API call to generate response.\n",
    "    if MODEL == 'gpt-4o-mini':\n",
    "        print(f'You are getting response from {MODEL}')\n",
    "        stream = openai.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            stream=True\n",
    "        )\n",
    "    else:\n",
    "        MODEL == 'llama3.2'\n",
    "        print(f'You are getting response from {MODEL}')\n",
    "        stream = ollama_via_openai.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            stream=True\n",
    "        )\n",
    "\n",
    "    # Initialize variables for response processing.\n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    \n",
    "    # Process the response stream and update display dynamically.\n",
    "    for chunk in stream:\n",
    "      \n",
    "         response += chunk.choices[0].delta.content or ''\n",
    "         update_display(Markdown(response), display_id=display_handle.display_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91262296-498c-4838-8552-061d2b93960f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29b2eb9c-ad4d-46bf-b5e8-219209c54ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are getting response from gpt-4o-mini\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Certainly! Let's break down the provided code snippet step by step for clarity:\n",
       "\n",
       "### Code Overview\n",
       "\n",
       "The code is a Python expression that utilizes the `yield from` statement along with a generator expression. Here’s the code for reference:\n",
       "\n",
       "```python\n",
       "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "```\n",
       "\n",
       "### Breakdown Steps\n",
       "\n",
       "1. **Understanding `yield from`:**\n",
       "   - The `yield from` statement is used in a generator function to yield all values from an iterable (like a list or a set) without needing to loop through it manually.\n",
       "   - It essentially delegates the yielding of values to another iterator.\n",
       "\n",
       "2. **Comprehension `{...}`:**\n",
       "   - This code uses a **set comprehension** to create a set (denoted by `{}`) of authors.\n",
       "   - A set is a collection of unique items, so any duplicate authors will appear only once in the resulting set.\n",
       "\n",
       "3. **The `for` Loop:**\n",
       "   - Inside the comprehension, we have a loop that iterates over `books`.\n",
       "   - `books` is presumably a list or collection of dictionaries where each dictionary represents a book (with attributes like \"author\").\n",
       "\n",
       "4. **The `if` Condition:**\n",
       "   - The condition `if book.get(\"author\")` checks if the `author` key exists in the `book` dictionary and if it holds a value that is not `None` or an empty string.\n",
       "   - This means that only books with a valid author will be included in the set.\n",
       "\n",
       "5. **The `get()` Method:**\n",
       "   - `book.get(\"author\")` is a method call that retrieves the value associated with the `author` key from each `book` dictionary.\n",
       "   - If the key is not found, `get()` will return `None`, ensuring that books without an author are excluded by the previous condition.\n",
       "\n",
       "### Summary of Functionality\n",
       "\n",
       "Putting it all together, the complete line of code does the following:\n",
       "\n",
       "1. Iterates over a collection of `books`.\n",
       "2. Extracts the `author` value for each book that has a valid `author`.\n",
       "3. Accumulates these authors into a **set**, ensuring all authors are unique.\n",
       "4. Yields each author from the set one by one.\n",
       "\n",
       "### Example\n",
       "\n",
       "Let's visualize this with an example:\n",
       "\n",
       "```python\n",
       "# Example list of books\n",
       "books = [\n",
       "    {\"title\": \"Book One\", \"author\": \"Author A\"},\n",
       "    {\"title\": \"Book Two\", \"author\": \"Author B\"},\n",
       "    {\"title\": \"Book Three\", \"author\": None},\n",
       "    {\"title\": \"Book Four\", \"author\": \"Author A\"},\n",
       "]\n",
       "\n",
       "# Generator function using your code\n",
       "def unique_authors():\n",
       "    yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "\n",
       "# Using the generator\n",
       "for author in unique_authors():\n",
       "    print(author)\n",
       "```\n",
       "\n",
       "### Expected Output\n",
       "\n",
       "For the above example, the output would be:\n",
       "\n",
       "```\n",
       "Author A\n",
       "Author B\n",
       "```\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "- The code effectively retrieves all unique authors from a list of books, skipping any that do not have an author value.\n",
       "- This approach is efficient and leverages Python’s set comprehensions and generator functionality effectively."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "askTutor(question=question, MODEL=MODEL_GPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b9dc17-5933-4aef-af9b-c0ea1d357c7c",
   "metadata": {},
   "source": [
    "2. Using both LLMs collaboratively approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e562e61a-8c94-41f7-b948-602a005f000f",
   "metadata": {},
   "source": [
    "I thought about like similar the idea of a RAG (Retrieval-Augmented Generation) approach, is an excellent idea to improve responses by refining the user query and producing a polished, detailed final answer. Two LLM talking each other its cool!!! Here's how we can implement this:\n",
    "Updated Concept:\n",
    "\n",
    "Refine Query with Ollama:\n",
    "Use Ollama to refine the raw user query into a well-structured prompt.\n",
    "This is especially helpful when users input vague or poorly structured queries.\n",
    "Generate Final Response with GPT:\n",
    "Pass the refined prompt from Ollama to GPT to generate the final, detailed, and polished response.\n",
    "Return the Combined Output:\n",
    "Combine the input, refined query, and the final response into a single display to ensure clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0133748-fe91-4efd-96d6-c0769f793761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_with_ollama(raw_question):\n",
    "    \"\"\"\n",
    "    Use Ollama to refine the user's raw question into a well-structured prompt.\n",
    "    \"\"\"\n",
    "    print(\"Refining the query using Ollama...\")\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant. Refine and structure the following user input.\"},\n",
    "\n",
    "        {\"role\": \"user\", \"content\": raw_question},\n",
    "    ]\n",
    "    response = ollama_via_openai.chat.completions.create(\n",
    "        model=MODEL_LLAMA,\n",
    "        messages=messages,\n",
    "        stream=False  # Non-streamed refinement\n",
    "    )\n",
    "    refined_query = response.choices[0].message.content\n",
    "    return refined_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ee61ec4-8583-4428-8267-d4c269e35884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_with_ollama_and_gpt(raw_question):\n",
    "    \"\"\"\n",
    "    Use Ollama to refine the user query and GPT to generate the final response.\n",
    "    \"\"\"\n",
    "    # Step 1: Refine the query using Ollama\n",
    "    refined_query = refine_with_ollama(raw_question)\n",
    "    \n",
    "    # Step 2: Generate final response with GPT\n",
    "    print(\"Generating the final response using GPT...\")\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": refined_query},\n",
    "    ]\n",
    "    stream = openai.chat.completions.create(\n",
    "        model=MODEL_GPT,\n",
    "        messages=messages,\n",
    "        stream=True  # Stream response for dynamic display\n",
    "    )\n",
    "\n",
    "    # Step 3: Combine responses\n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(f\"### Refined Query:\\n\\n{refined_query}\\n\\n---\\n\\n### Final Response:\"), display_id=True)\n",
    "    for chunk in stream:\n",
    "        response_chunk = chunk.choices[0].delta.content or \"\"\n",
    "\n",
    "        update_display(Markdown(f\"### Refined Query:\\n\\n{refined_query}\\n\\n---\\n\\n### Final Response:\\n\\n{response}\"), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0220af6-a360-4db5-8bae-294ac578f0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage\n",
    "question = \"\"\"\n",
    "Please explain what this code does:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c75e19-394f-4f2c-8d49-a5a6b3f9809a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refining the query using Ollama...\n"
     ]
    }
   ],
   "source": [
    "ask_with_ollama_and_gpt(raw_question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644b8c0c-3a1b-430d-9db3-84efc49784bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
